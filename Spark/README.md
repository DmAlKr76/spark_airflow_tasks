> # Работа с данными в Spark
Рыботы выполнены в среде Apache Zeppelin на развернутом кластере.


- job_1 - задание на фильтрацию конфигураций;
- job_2 - подсчет суточных агрегатов и сохранение данных в HDFS;
- job_3 - создание графиков;
- job_4 - оптимизация запросов и работа с перекосом данных (data skew);
- job_5 - работа с данными для их анализа и витрин.