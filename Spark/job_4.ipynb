{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.conf\n",
        "\n",
        "spark.executor.instances=2\n",
        "spark.executor.memory=1G\n",
        "spark.kryoserializer.buffer.max=1024m\n",
        "\n",
        "spark.sql.autoBroadcastJoinThreshold=20971520"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Генерация events таблицы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.mllib.random.RandomRDDs._\n",
        "import java.time.LocalDate\n",
        "import java.time.format.DateTimeFormatter\n",
        "\n",
        "val dates = (0 to 14).map(LocalDate.of(2020, 11, 1).plusDays(_).format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))).toSeq\n",
        "\n",
        "def generateCity(r: Double): String = if (r < 0.9) \"BIG_CITY\" else \"SMALL_CITY_\" + scala.math.round((r - 0.9) * 1000)\n",
        "\n",
        "def generateCityUdf = udf(generateCity _)\n",
        "\n",
        "// spark.sql(\"drop table hw2.events_full\")\n",
        "spark.sql(\"create database hw_4\")\n",
        "for(i <- dates) {\n",
        "    uniformRDD(sc, 10000000L, 1)\n",
        "    .toDF(\"uid\")\n",
        "    .withColumn(\"date\", lit(i))\n",
        "    .withColumn(\"city\", generateCityUdf($\"uid\"))\n",
        "    .selectExpr(\"date\", \" sha2(cast(uid as STRING), 256) event_id\", \"city\")\n",
        "    .withColumn(\"skew_key\", when($\"city\" === \"BIG_CITY\", lit(\"big_event\")).otherwise($\"event_id\"))\n",
        "    .write.mode(\"append\")\n",
        "    .partitionBy(\"date\")\n",
        "    .saveAsTable(\"hw_4.events_full\")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Генерация events_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "spark.table(\"hw_4.events_full\")\n",
        ".select(\"event_id\")\n",
        ".sample(0.001)\n",
        ".repartition(2)\n",
        ".write.mode(\"overwrite\")\n",
        ".saveAsTable(\"hw_4.sample\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Генерация events_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "spark.table(\"hw_4.sample\")\n",
        ".limit(100)\n",
        ".coalesce(1)\n",
        ".write.mode(\"overwrite\")\n",
        ".saveAsTable(\"hw_4.sample_small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Генерация events_big"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "spark.table(\"hw_4.events_full\")\n",
        ".select(\"event_id\")\n",
        ".sample(0.003)\n",
        ".repartition(1)\n",
        ".write.mode(\"overwrite\")\n",
        ".saveAsTable(\"hw_4.sample_big\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Генерация events_very_big"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "spark.table(\"hw_4.events_full\")\n",
        ".select(\"event_id\")\n",
        ".sample(0.015)\n",
        ".repartition(1)\n",
        ".write.mode(\"overwrite\")\n",
        ".saveAsTable(\"hw_4.sample_very_big\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сгрененирован большой набор синтетических данных в таблице hw2.events_full. Из этого набора данных созданы маленькие (относительно исходного набора) таблицы разного размера kotelnikov.sample_[small, big, very_big]. \n",
        "\n",
        "Ответить на вопросы:\n",
        " * какова структура таблиц\n",
        " * сколько в них записей \n",
        " * сколько места занимают данные\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "tables = spark.sql(\"SHOW TABLES IN hw_4\").collect()\n",
        "for table in tables:\n",
        "    print(f'Схема таблицы {table[0]}.{table[1]}:')\n",
        "    spark.table(f'{table[0]}.{table[1]}').printSchema()\n",
        "    print(f\"Число записей в таблице {table[0]}.{table[1]}: {spark.table(f'{table[0]}.{table[1]}').count()}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Получить планы запросов для джойна большой таблицы hw_4.events_full с каждой из таблиц hw_4.sample, hw_4.sample_big, hw_4.sample_very_big по полю event_id. В каких случаях используется BroadcastHashJoin? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "tables = spark.sql(\"SHOW TABLES IN hw_4\").collect()\n",
        "for table in tables:\n",
        "    if table[1] in ['sample', 'sample_big', 'sample_very_big']:\n",
        "        print(f'План запроса для таблицы hw_4.events_full с таблией {table[0]}.{table[1]}:')\n",
        "        spark.table('hw_4.events_full').join(\n",
        "            spark.table(f'{table[0]}.{table[1]}'), \n",
        "            on='event_id', \n",
        "            how='inner'\n",
        "            ).explain()\n",
        "        print(\"-------------------------------------------------\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "print(f\"Значение по умолчанию параметра autoBroadcastJoinThreshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BroadcastHashJoin используется только при джоине с таблицей hw_4.sample, так как он размером 7 Кб, а размер по умолчанию autoBroadcastJoinThreshold 10 Мб. Размер остальных таблиц больше 10 Мб."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Выполнить джойны с таблицами  hw_4.sample,  hw_4.sample_big в отдельных параграфах, чтобы узнать время выполнения запросов (например, вызвать .count() для результатов запросов). Время выполнения параграфа считается автоматически и указывается в нижней части по завершении\n",
        "\n",
        "Зайти в spark ui (ссылку сгенерировать в следующем папраграфе). Сколько tasks создано на каждую операцию? Почему именно столько? Каков DAG вычислений?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"events_full join sample\")\n",
        "\n",
        "spark.table('hw_4.events_full').join(\n",
        "            spark.table('hw_4.sample'), \n",
        "            on='event_id', \n",
        "            how='inner'\n",
        "            ).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"events_full join sample_big\")\n",
        "\n",
        "spark.table('hw_4.events_full').join(\n",
        "            spark.table('hw_4.sample_big'), \n",
        "            on='event_id', \n",
        "            how='inner'\n",
        "            ).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "82 таски для sample (44 секунды) и 284 для sample_big (2 мин. 8 сек.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Насильный broadcast\n",
        "\n",
        "Оптимизировать джойн с таблицами hw_4.sample_big, hw_4.sample_very_big с помощью broadcast(df). Выполнить запрос, посмотреть в UI, как поменялся план запроса, DAG, количество тасков. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"events_full join sample_big with broadcast\")\n",
        "\n",
        "spark.table('hw_4.events_full').join(\n",
        "            broadcast(spark.table('hw_4.sample_big')), \n",
        "            on='event_id', \n",
        "            how='inner'\n",
        "            ).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"events_full join sample_very_big with broadcast\")\n",
        "\n",
        "spark.table('hw_4.events_full').join(\n",
        "            broadcast(spark.table('hw_4.sample_very_big')), \n",
        "            on='event_id', \n",
        "            how='inner'\n",
        "            ).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "82 таски для sample_big (1 мин 20 сек.), для sample_very_big с принудительным бродкастом не полняется, так как sample_very_big не помещается на каждой ноде, возникает исключение(org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Отключение auto broadcast\n",
        "\n",
        "Отключить автоматический броадкаст командой spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\"). Сделать джойн с семплом hw_4.sample, сравнить время выполнения запроса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"events_full join sample without broadcast\")\n",
        "\n",
        "spark.table('hw_4.events_full').join(\n",
        "            spark.table('hw_4.sample'), \n",
        "            on='event_id', \n",
        "            how='inner'\n",
        "            ).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "284 таски для sample (4 мин. 16 сек.) с отключенным бродкастом по сравнению с 44 секундами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#Вернуть настройку к исходной\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "spark.sql(\"clear cache\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В процессе обработки данных может возникнуть перекос объёма партиций по количеству данных (data skew). В таком случае время выполнения запроса может существенно увеличиться, так как данные распределятся по исполнителям неравномерно. \n",
        "\n",
        "Инициализация датафрейма."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark \n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "skew_df = spark.table(\"hw_4.events_full\")\\\n",
        ".where(\"date = '2020-11-01'\")\\\n",
        ".repartition(30, col(\"city\"))\\\n",
        ".cache()\n",
        "\n",
        "skew_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Датафрейм разделен на 30 партиций по ключу city, который имеет сильно  неравномерное распределение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1. Наблюдение проблемы\n",
        "\n",
        "Посчитать количество event_count различных событий event_id , содержащихся в skew_df с группировкой по городам. Результат упорядочить по event_count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.functions import count, col\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"example of a problem\")\n",
        "\n",
        "grouped = skew_df.groupBy(\"city\")\\\n",
        "                 .agg(count(\"event_id\")\\\n",
        "                 .alias(\"event_count\"))\\\n",
        "                 .orderBy(col(\"event_count\").desc())\n",
        "\n",
        "grouped.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В spark ui stage джобы, состоящем из 30 тасков (из такого количества партиций состоит skew_df) можно увидеть время выполнения тасков по экзекьюторам. Одному из них выпала партиция с существенно большим количеством данных. Остальные экзекьюторы в это время бездействуют - это является проблемой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2. repartition\n",
        "\n",
        "Предварительное перемешивание данных с помощью метода repartition(p_num), где p_num - количество партиций, на которые будет перемешан исходный датафрейм"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"repartition\")\n",
        "\n",
        "grouped = skew_df.repartition(100)\\\n",
        "                 .groupBy(\"city\")\\\n",
        "                 .agg(f.count(\"event_id\")\\\n",
        "                 .alias(\"event_count\"))\\\n",
        "                 .orderBy(f.col(\"event_count\").desc())\n",
        "\n",
        "grouped.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.3. Key Salting\n",
        "\n",
        "Другой способ - создание синтетического ключа с равномерным распределением. В нашем случае неравномерность исходит от единственного значения city='BIG_CITY', которое часто повторяется в данных и при группировке попадает к одному экзекьютору. В таком случае лучше провести группировку в два этапа по синтетическому ключу CITY_SALT, который принимает значение BIG_CITY_rand (rand - случайное целое число) для популярного значения BIG_CITY и CITY для остальных значений. На втором этапе восстанавливаем значения CITY и проводим повторную агрегацию, которая не занимает времени, потому что проводится по существенно меньшего размера данным. \n",
        "\n",
        "Такая же техника применима и к джойнам по неравномерному ключу, см, например https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8\n",
        "\n",
        "Что нужно реализовать:\n",
        "* добавить синтетический ключ\n",
        "* группировка по синтетическому ключу\n",
        "* восстановление исходного значения\n",
        "* группировка по исходной колонке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# Добавляем колонку с \"солью\": для BIG_CITY - случайное целое от 0 до 20 включительно, для SMALL_CITY - 21, \n",
        "# Выводим кусочки датафрейма для BIG_CITY и SMALL_CITY для контроля правильности выполненной процедуры\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"Key Salting (salt)\")\n",
        "\n",
        "BIG_CITY_rand = F.expr(\"\"\"pmod(round(rand() * 100, 0), 20)\"\"\").cast(\"integer\")\n",
        "SMALL_CITY = 21\n",
        "\n",
        "salted = skew_df.withColumn(\"salt\", F.when(F.col(\"city\") == \"BIG_CITY\", BIG_CITY_rand).otherwise(SMALL_CITY))\n",
        "\n",
        "salted.filter(salted.city==\"BIG_CITY\").show()\n",
        "salted.filter(salted.city!=\"BIG_CITY\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "sc.setLocalProperty(\"callSite.short\", \"Key Salting (grouping)\")\n",
        "\n",
        "result = salted.groupBy(F.col(\"city\"), F.col(\"salt\")).agg(F.count(\"*\").alias(\"count\")) \\\n",
        "               .groupBy(F.col(\"city\")).agg(F.sum(\"count\").alias(\"event_count\")) \\\n",
        "               .orderBy(F.col(\"event_count\").desc())\n",
        "      \n",
        "result.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "homework_4"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
