{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "market_events = spark.table(\"market.events\")\n",
        "# market_events = spark.table(\"kruglov.market_events\")\n",
        "# market_events = spark.read.parquet(\"/apps/hive/warehouse/eakotelnikov.db/market_events\")\n",
        "\n",
        "market_events.show(5)\n",
        "market_events.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Вывести топ категорий по количеству просмотров товаров за всё время"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# вывести в виде\n",
        "# +--------------------------------+----------+\n",
        "# |category_code                   |view_count|\n",
        "# +--------------------------------+----------+\n",
        "# |null                            |20837460  |\n",
        "# |electronics.smartphone          |14832387  |\n",
        "# |computers.notebook              |2103024   |\n",
        "\n",
        "from pyspark.sql.functions import count, col\n",
        "\n",
        "market_events.filter(col(\"event_type\") == \"view\") \\\n",
        "             .groupBy(\"category_code\") \\\n",
        "             .agg(count(\"*\").alias(\"view_count\")) \\\n",
        "             .orderBy(col(\"view_count\").desc()) \\\n",
        "             .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Построить гистограмму распределения цен на проданные товары за 10 октября"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# результат визуализировать через z.show()\n",
        "\n",
        "from pyspark.sql.functions import count, col\n",
        "\n",
        "hist_data = market_events.filter((col(\"event_date\") == \"2019-10-10\") & (col(\"event_type\") == \"purchase\")) \\\n",
        "                         .groupBy(\"price\") \\\n",
        "                         .agg(count(\"*\").alias(\"count\")) \\\n",
        "                         .orderBy(col(\"price\").asc())\n",
        "                         \n",
        "hist_data.show()\n",
        "z.show(hist_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Решение с бинами через бакетирование\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "# Фильтруем датафрейм\n",
        "filtered_data = market_events.filter((F.col(\"event_date\") == \"2019-10-10\") & (F.col(\"event_type\") == \"purchase\"))\n",
        "\n",
        "# Определяем границы диапазонов\n",
        "bins = 20\n",
        "max_price = filtered_data.agg({\"price\": \"max\"}).collect()[0][0]\n",
        "step = max_price / bins\n",
        "boundaries = [i * step for i in range(bins+1)]\n",
        "\n",
        "# Создаем Bucketizer\n",
        "bucketizer = Bucketizer(splits=boundaries, inputCol=\"price\", outputCol=\"price_bucket\")\n",
        "\n",
        "# Применяем Bucketizer к датафрейму\n",
        "filtered_data = bucketizer.transform(filtered_data)\n",
        "\n",
        "hist_data = filtered_data.select(\"price\", \"price_bucket\") \\\n",
        "                          .groupBy(\"price_bucket\") \\\n",
        "                          .agg(F.count(\"*\").alias(\"bucket_count\")) \\\n",
        "                          .orderBy(F.col(\"price_bucket\").asc())\n",
        "            \n",
        "\n",
        "# Выводим результат\n",
        "print(f\"Шаг бакета {step} руб.\") # шаг каждого бакета\n",
        "hist_data.show()\n",
        "z.show(hist_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Посчитать количество продаж за октябрь отдельно бренда apple и остальных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# вывести через df.show() в виде\n",
        "# +--------+------+\n",
        "# |is_apple| count|\n",
        "# +--------+------+\n",
        "# |    true| *****|\n",
        "# |   false| *****|\n",
        "# +--------+------+\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "apple_oct_data = market_events.filter((F.month(\"event_date\") == 10) & (col(\"event_type\") == \"purchase\")) \\\n",
        "                              .na.fill({\"brand\": \"No_brand\"}) \\\n",
        "                              .groupBy(\"brand\") \\\n",
        "                              .agg(count(\"*\").alias(\"brand_count\")) \\\n",
        "                              .withColumn(\"is_apple\", col(\"brand\") == \"apple\") \\\n",
        "                              .groupBy(\"is_apple\") \\\n",
        "                              .agg(F.sum(\"brand_count\").alias(\"count\"))\n",
        "\n",
        "apple_oct_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Построить почасовой график продаж и прибыли за вторую неделю октября"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# z.show(), ключ -- часы, значения -- число продаж и сумма прибыли за этот час\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "data_by_hour = market_events.filter((F.weekofyear(\"event_date\") == 41) & (F.col(\"event_type\") == \"purchase\")) \\\n",
        "                .withColumn(\"hour\", F.hour(\"event_time\")) \\\n",
        "                .groupBy(\"hour\") \\\n",
        "                .agg(F.count(\"price\").alias(\"sales_count\"), F.sum(\"price\").alias(\"hour_revenue\"))\n",
        "\n",
        "data_by_hour.show()\n",
        "z.show(data_by_hour)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Построить почасовой график продаж и прибыли за вторую неделю октября"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# z.show(), ключ -- час в диапазоне от 0 до 23, значение -- усредненное за месяц число продаж в этот час на месячных данных\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "avg_data_by_hour = market_events.filter((F.month(\"event_date\") == 10) & (F.col(\"event_type\") == \"purchase\")) \\\n",
        "                        .withColumn(\"hour\", F.hour(\"event_time\")) \\\n",
        "                        .withColumn(\"day\", F.dayofmonth(\"event_date\")) \\\n",
        "                        .groupBy([\"hour\", \"day\"]) \\\n",
        "                        .agg(F.count(\"price\").alias(\"sales_count\"), F.sum(\"price\").alias(\"hour_revenue\")) \\\n",
        "                        .groupBy([\"hour\"]) \\\n",
        "                        .agg(F.avg(\"sales_count\").alias(\"avg_sales_count\"), F.avg(\"hour_revenue\").alias(\"avg_hour_revenue\"))\n",
        "                        \n",
        "avg_data_by_hour.show()\n",
        "z.show(avg_data_by_hour)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "homework_3"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
