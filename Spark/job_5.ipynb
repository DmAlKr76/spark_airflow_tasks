{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сгруппировать по ключу, просуммировать значения, вывести результат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5), (3, 4), (1, 5), (4, 1)])\n",
        "\n",
        "result = rdd.groupByKey()\\\n",
        "            .map(lambda x:(x[0], sum(x[1])) )\\\n",
        "            .collect()\n",
        "\n",
        "for (k, v) in result:\n",
        "    print(f'For key {k} sum of values {v}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Посчитать частоту встречаемости слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "lines = sc.parallelize([\n",
        "    \"a ab abc\",\n",
        "    \"a ac abc\",\n",
        "    \"b b ab abc\"\n",
        "    ])\n",
        "\n",
        "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
        "              .map(lambda x: (x, 1))\\\n",
        "              .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "output = counts.collect()\n",
        "\n",
        "for (word, count) in output:\n",
        "    print(\"%s: %i\" % (word, count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# market.events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Добавить колонки category_1, category_2, category_3 с категориями различного уровня"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "market_events = spark.table(\"market.events\")\n",
        "df_with_category = market_events.withColumn(\"category_1\", F.when(market_events.category_code.isNotNull(), F.split(\"category_code\", \"\\\\.\")[0]).otherwise(None)) \\\n",
        "                      .withColumn(\"category_2\", F.when(market_events.category_code.isNotNull() & (F.size(F.split(\"category_code\", \"\\\\.\")) > 1), \n",
        "                                                    F.split(\"category_code\", \"\\\\.\")[1]).otherwise(None)) \\\n",
        "                      .withColumn(\"category_3\", F.when(market_events.category_code.isNotNull() & (F.size(F.split(\"category_code\", \"\\\\.\")) > 2), \n",
        "                                                    F.split(\"category_code\", \"\\\\.\")[2]).otherwise(None))\n",
        "\n",
        "df_with_category.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Более простое решение:\n",
        "\n",
        "market_events.withColumn(\"category_1\", F.split(\"category_code\", \"\\\\.\")[0]) \\\n",
        "             .withColumn(\"category_2\", F.split(\"category_code\", \"\\\\.\")[1]) \\\n",
        "             .withColumn(\"category_3\", F.split(\"category_code\", \"\\\\.\")[2]) \\\n",
        "             .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Вывести топ-3 брендов по количеству просмотров для каждой категории 2-го уровня"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "top_views_cat_2 = df_with_category.filter((df_with_category.event_type == 'view') & (df_with_category.brand.isNotNull())) \\\n",
        "                                  .groupBy('category_2', 'brand') \\\n",
        "                                  .agg(F.count(\"*\").alias(\"views\")) \\\n",
        "                                  .withColumn('rank', F.row_number().over(Window.partitionBy('category_2').orderBy(F.desc('views')))) \\\n",
        "                                  .filter(F.col('rank') <= 3) \\\n",
        "                                  .orderBy('category_2', 'rank')\n",
        "\n",
        "top_views_cat_2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Датасет с треками"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "создание hw_3.tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "sch=ArrayType(StringType());\n",
        "\n",
        "# важно что разделитель ', ' с пробелом, иначе пробелы добавятся в значения\n",
        "tracks = spark.read.option(\"header\", \"true\") \\\n",
        "        .option(\"escape\", '\"') \\\n",
        "        .option(\"InferSchema\", \"true\") \\\n",
        "        .csv(\"/datasets/tracks.csv\") \\\n",
        "        .withColumn(\"release_year\", f.substring(\"release_date\", 1, 4).cast(IntegerType())) \\\n",
        "        .withColumn(\"array_artist\", f.split(f.regexp_replace(f.col(\"artists\"), \"[\\]\\[\\']\", \"\"),\", \")) \\\n",
        "        .cache() #выделяем год в отдельную колонку и преобразуем колонку с артистами в массив\n",
        "\n",
        "tracks_exp = tracks.select(  \n",
        "                            \"name\", \n",
        "                            \"popularity\",\n",
        "                            \"danceability\",\n",
        "                            \"energy\",\n",
        "                            \"speechiness\",\n",
        "                            \"acousticness\",\n",
        "                            \"liveness\",\n",
        "                            \"valence\",\n",
        "                            \"release_year\",\n",
        "                            \"artists\",\n",
        "                            f.explode(f.col(\"array_artist\") ).alias(\"name_artist\")\n",
        "                        ) #создаем отдельную таблицу с развернутым массивом артистов\n",
        "                        \n",
        "tracks_exp.printSchema()\n",
        "\n",
        "spark.sql(\"create database hw_3\")\n",
        "tracks_exp.write.mode(\"overwrite\").saveAsTable(\"hw_3.tracks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "tracks = spark.table(\"hw_3.tracks\")\n",
        "z.show(tracks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Какие артисты выпустили наибольшее число песен из годового топ-100 (по популярности)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "tracks = spark.table(\"hw_3.tracks\")\n",
        "\n",
        "tracks = tracks.dropDuplicates()\n",
        "\n",
        "window_years = Window.partitionBy(\"release_year\").orderBy(F.desc(\"popularity\"))\n",
        "\n",
        "top_songs_count_artists = tracks.withColumn(\"rank\", F.row_number().over(window_years)) \\\n",
        "                                .filter(F.col(\"rank\") <= 100) \\\n",
        "                                .groupBy(\"name_artist\") \\\n",
        "                                .count() \\\n",
        "                                .orderBy(F.desc(\"count\"))\n",
        "\n",
        "top_songs_count_artists.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Вывести топ артистов, которые чаще других попадали в годовой топ-100 песен по популярности?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "   \n",
        "window_years = Window.partitionBy(\"release_year\").orderBy(F.desc(\"popularity\"))\n",
        "\n",
        "top_artists_count = tracks.withColumn(\"rank\", F.row_number().over(window_years)) \\\n",
        "                          .filter(F.col(\"rank\") <= 100) \\\n",
        "                          .groupBy(\"release_year\", \"name_artist\") \\\n",
        "                          .agg(F.countDistinct(F.col(\"name_artist\")).alias(\"name_artist_count\")) \\\n",
        "                          .groupBy(\"name_artist\") \\\n",
        "                          .agg(F.count(\"*\").alias(\"count_artists\")) \\\n",
        "                          .orderBy(F.desc(\"count_artists\"))\n",
        "\n",
        "top_artists_count.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Какие артисты дольше других несколько лет подряд держались в ежегодном топ-100 песен по популярности?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "window_years = Window.partitionBy(\"release_year\").orderBy(F.desc(\"popularity\"))\n",
        "window_artists = Window.partitionBy(\"name_artist\").orderBy(\"release_year\")\n",
        "\n",
        "top_artists_longer = tracks.withColumn(\"rank\", F.row_number().over(window_years)) \\\n",
        "                          .filter(F.col(\"rank\") <= 100) \\\n",
        "                          .drop(F.col(\"rank\")) \\\n",
        "                          .select(F.col(\"name_artist\"), F.col(\"release_year\")) \\\n",
        "                          .dropDuplicates() \\\n",
        "                          .withColumn(\"group\", F.col(\"release_year\") - F.row_number().over(window_artists)) \\\n",
        "                          .groupBy(\"name_artist\", \"group\") \\\n",
        "                          .agg(F.count(\"*\").alias(\"num_years\")) \\\n",
        "                          .drop(F.col(\"group\")) \\\n",
        "                          .orderBy(F.desc(\"num_years\"))\n",
        "\n",
        "top_artists_longer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Решение с udf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "window_years = Window.partitionBy(\"release_year\").orderBy(F.desc(\"popularity\"))\n",
        "\n",
        "top_songs_by_year = tracks.withColumn(\"rank\", F.row_number().over(window_years)) \\\n",
        "                          .filter(F.col(\"rank\") <= 100) \\\n",
        "                          .drop(F.col(\"rank\"))\n",
        "\n",
        "# Подсчет количества лет подряд для каждого артиста\n",
        "@udf(returnType=IntegerType())\n",
        "def count_consecutive_years(release_years):\n",
        "    consecutive_years = 0\n",
        "    prev_year = None\n",
        "    for year in sorted(release_years):\n",
        "        if prev_year is not None and year == prev_year + 1:\n",
        "            consecutive_years += 1\n",
        "        prev_year = year\n",
        "    return consecutive_years\n",
        "\n",
        "\n",
        "top_artists_longer = top_songs_by_year.groupBy(\"name_artist\") \\\n",
        "                                      .agg(F.collect_list(\"release_year\").alias(\"release_years\")) \\\n",
        "                                      .withColumn(\"max_period\", count_consecutive_years(F.col(\"release_years\"))) \\\n",
        "                                      .filter(F.col(\"max_period\") >= 2) \\\n",
        "                                      .select(F.col(\"name_artist\"), F.col(\"max_period\")) \\\n",
        "                                      .orderBy(F.desc(\"max_period\"))\n",
        "\n",
        "top_artists_longer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Через lag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# https://stackoverflow.com/questions/56384625/pyspark-cumulative-sum-with-reset-condition\n",
        "\n",
        "window_years = Window.partitionBy(\"release_year\").orderBy(F.desc(\"popularity\"))\n",
        "window_artists = Window.partitionBy(\"name_artist\").orderBy(\"release_year\")\n",
        "\n",
        "top_artists_longer = tracks.withColumn(\"rank\", F.row_number().over(window_years)) \\\n",
        "                           .filter(F.col(\"rank\") <= 100) \\\n",
        "                           .withColumn(\"prev_release_year\", F.lag(F.col(\"release_year\")).over(window_artists)) \\\n",
        "                           .withColumn(\"consecutive_years\", F.when(F.col(\"release_year\") - F.col(\"prev_release_year\") == 1, 1).otherwise(0)) \\\n",
        "                           .withColumn(\"period\", F.sum(\"consecutive_years\").over(window_artists)) \\\n",
        "                           .filter(F.col(\"period\") >= 2) \\\n",
        "                           .groupBy(F.col(\"name_artist\")) \\\n",
        "                           .agg(F.max(\"period\").alias(\"max_period\")) \\\n",
        "                           .orderBy(F.desc(\"max_period\"))\n",
        "top_artists_longer.show()                           "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для каждой аудиохарактеристики вывести топ 3 артистов по среднему значению каждой аудиохарактеристики его песен. Дополнительно: отнормировать на среднее значение аудиохарактеристики в год выхода песен."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "window_danceability = Window.orderBy(F.desc(\"avg_danceability\"))\n",
        "window_acousticness = Window.orderBy(F.desc(\"avg_acousticness\"))\n",
        "window_energy = Window.orderBy(F.desc(\"avg_energy\"))\n",
        "window_speechiness = Window.orderBy(F.desc(\"avg_speechiness\"))\n",
        "window_liveness = Window.orderBy(F.desc(\"avg_liveness\"))\n",
        "window_valence = Window.orderBy(F.desc(\"avg_valence\"))\n",
        "\n",
        "\n",
        "avg_by_artist = tracks.groupBy(\"name_artist\") \\\n",
        "                      .agg(F.avg(\"danceability\").alias(\"avg_danceability\"),\n",
        "                           F.avg(\"acousticness\").alias(\"avg_acousticness\"),\n",
        "                           F.avg(\"energy\").alias(\"avg_energy\"),\n",
        "                           F.avg(\"speechiness\").alias(\"avg_speechiness\"),\n",
        "                           F.avg(\"liveness\").alias(\"avg_liveness\"),\n",
        "                           F.avg(\"valence\").alias(\"avg_valence\"))\n",
        "\n",
        "top_danceability = avg_by_artist.withColumn(\"rank\", F.row_number().over(window_danceability)) \\\n",
        "                                .filter(F.col(\"rank\") <= 3) \\\n",
        "                                .select(F.col(\"name_artist\"), F.col(\"avg_danceability\"))\n",
        "\n",
        "top_acousticness = avg_by_artist.withColumn(\"rank\", F.row_number().over(window_acousticness)) \\\n",
        "                                .filter(F.col(\"rank\") <= 3) \\\n",
        "                                .select(F.col(\"name_artist\"), F.col(\"avg_acousticness\"))\n",
        "\n",
        "top_energy = avg_by_artist.withColumn(\"rank\", F.row_number().over(window_energy)) \\\n",
        "                          .filter(F.col(\"rank\") <= 3) \\\n",
        "                          .select(F.col(\"name_artist\"), F.col(\"avg_energy\"))\n",
        "\n",
        "top_speechiness = avg_by_artist.withColumn(\"rank\", F.row_number().over(window_speechiness)) \\\n",
        "                               .filter(F.col(\"rank\") <= 3) \\\n",
        "                               .select(F.col(\"name_artist\"), F.col(\"avg_speechiness\"))\n",
        "\n",
        "top_liveness = avg_by_artist.withColumn(\"rank\", F.row_number().over(window_liveness)) \\\n",
        "                            .filter(F.col(\"rank\") <= 3) \\\n",
        "                            .select(F.col(\"name_artist\"), F.col(\"avg_liveness\"))\n",
        "\n",
        "top_valence = avg_by_artist.withColumn(\"rank\", F.row_number().over(window_valence)) \\\n",
        "                           .filter(F.col(\"rank\") <= 3) \\\n",
        "                           .select(F.col(\"name_artist\"), F.col(\"avg_valence\"))\n",
        "                           \n",
        "top_danceability.show()\n",
        "top_acousticness.show()\n",
        "top_energy.show()\n",
        "top_speechiness.show()\n",
        "top_liveness.show()\n",
        "top_valence.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "    \n",
        "avg_by_year = tracks.groupBy(\"release_year\") \\\n",
        "                      .agg(F.avg(\"danceability\").alias(\"avg_danceability\"),\n",
        "                           F.avg(\"acousticness\").alias(\"avg_acousticness\"),\n",
        "                           F.avg(\"energy\").alias(\"avg_energy\"),\n",
        "                           F.avg(\"speechiness\").alias(\"avg_speechiness\"),\n",
        "                           F.avg(\"liveness\").alias(\"avg_liveness\"),\n",
        "                           F.avg(\"valence\").alias(\"avg_valence\"))\n",
        "                           \n",
        "min_by_year = tracks.groupBy(\"release_year\") \\\n",
        "                      .agg(F.min(\"danceability\").alias(\"min_danceability\"),\n",
        "                           F.min(\"acousticness\").alias(\"min_acousticness\"),\n",
        "                           F.min(\"energy\").alias(\"min_energy\"),\n",
        "                           F.min(\"speechiness\").alias(\"min_speechiness\"),\n",
        "                           F.min(\"liveness\").alias(\"min_liveness\"),\n",
        "                           F.min(\"valence\").alias(\"min_valence\"))\n",
        "                           \n",
        "max_by_year = tracks.groupBy(\"release_year\") \\\n",
        "                      .agg(F.max(\"danceability\").alias(\"max_danceability\"),\n",
        "                           F.max(\"acousticness\").alias(\"max_acousticness\"),\n",
        "                           F.max(\"energy\").alias(\"max_energy\"),\n",
        "                           F.max(\"speechiness\").alias(\"max_speechiness\"),\n",
        "                           F.max(\"liveness\").alias(\"max_liveness\"),\n",
        "                           F.max(\"valence\").alias(\"max_valence\"))\n",
        "                           \n",
        "df_with_agg = tracks.join(F.broadcast(avg_by_year), \"release_year\") \\\n",
        "                    .join(F.broadcast(min_by_year), \"release_year\") \\\n",
        "                    .join(F.broadcast(max_by_year), \"release_year\")\n",
        "                    \n",
        "                                            \n",
        "normalized_df = df_with_agg.withColumn(\"norm_danceability\", (F.col(\"danceability\") - F.col(\"avg_danceability\")) / (F.col(\"max_danceability\") - F.col(\"min_danceability\"))) \\\n",
        "                           .withColumn(\"norm_acousticness\", (F.col(\"acousticness\") - F.col(\"avg_acousticness\")) / (F.col(\"max_acousticness\") - F.col(\"min_acousticness\"))) \\\n",
        "                           .withColumn(\"norm_energy\", (F.col(\"energy\") - F.col(\"avg_energy\")) / (F.col(\"max_energy\") - F.col(\"min_energy\"))) \\\n",
        "                           .withColumn(\"norm_speechiness\", (F.col(\"speechiness\") - F.col(\"avg_speechiness\")) / (F.col(\"max_speechiness\") - F.col(\"min_speechiness\"))) \\\n",
        "                           .withColumn(\"norm_liveness\", (F.col(\"liveness\") - F.col(\"avg_liveness\")) / (F.col(\"max_liveness\") - F.col(\"min_liveness\"))) \\\n",
        "                           .withColumn(\"norm_valence\", (F.col(\"valence\") - F.col(\"avg_valence\")) / (F.col(\"max_valence\") - F.col(\"min_valence\"))) \\\n",
        "                           .select(F.col(\"release_year\"), F.col(\"norm_danceability\"), F.col(\"norm_acousticness\"), F.col(\"norm_energy\"), F.col(\"norm_speechiness\"), F.col(\"norm_liveness\"), F.col(\"norm_valence\")) \\\n",
        "                           #.orderBy(F.col(\"release_year\"))\n",
        "\n",
        "normalized_df.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "homework_5"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
